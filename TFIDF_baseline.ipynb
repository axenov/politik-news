{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "from nlp import load_dataset\n",
    "dataset = load_dataset('de_politik_news.py', cache_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "vectorizer = TfidfVectorizer(tokenizer = word_tokenize)\n",
    "X_train = vectorizer.fit_transform(dataset['train']['text'])\n",
    "y_train = dataset['train']['class']\n",
    "X_test = vectorizer.transform(dataset['test']['text'])\n",
    "y_test = dataset['test']['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "#class_test = dataset['test']['class']\n",
    "def evaluate(model_test, class_test):\n",
    "    accuracy = accuracy_score(class_test, model_test)\n",
    "    f1_micro = f1_score(class_test, model_test, average = 'micro')\n",
    "    f1_macro = f1_score(class_test, model_test, average = 'macro')\n",
    "    report = classification_report(class_test, model_test)\n",
    "    print(f'accuracy: {accuracy}')\n",
    "    print(f'F1-micro: {f1_micro}')\n",
    "    print(f'F1-macro: {f1_macro}')\n",
    "    print(f'Report: {report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=30, min_impurity_decrease = 0.001, \n",
    "                               criterion = \"entropy\", n_estimators=1000, \n",
    "                               class_weight='balanced', random_state=0)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4257964257964258\n",
      "F1-micro: 0.4257964257964258\n",
      "F1-macro: 0.43026245023451065\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "      center       0.31      0.23      0.27      1349\n",
      " center-left       0.34      0.38      0.36      1159\n",
      "center-right       0.51      0.55      0.53      1754\n",
      "    far-left       0.59      0.40      0.48       215\n",
      "   far-right       0.46      0.58      0.51       671\n",
      "\n",
      "    accuracy                           0.43      5148\n",
      "   macro avg       0.44      0.43      0.43      5148\n",
      "weighted avg       0.42      0.43      0.42      5148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('models/RF/model.bin', 'wb'))\n",
    "model = pickle.load(open('models/RF/model.bin', 'rb'))\n",
    "\n",
    "pickle.dump(vectorizer, open('models/RF/vectorizer.bin', 'wb'))\n",
    "vectorizer = pickle.load(open('models/RF/vectorizer.bin', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "model = BalancedRandomForestClassifier(max_depth=20, min_impurity_decrease = 0.001, \n",
    "                               criterion = \"entropy\", n_estimators=1000,  random_state=0)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4065656565656566\n",
      "F1-micro: 0.4065656565656566\n",
      "F1-macro: 0.4127118842652152\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "      center       0.30      0.21      0.24      1349\n",
      " center-left       0.30      0.26      0.28      1159\n",
      "center-right       0.52      0.54      0.53      1754\n",
      "    far-left       0.43      0.65      0.52       215\n",
      "   far-right       0.41      0.62      0.49       671\n",
      "\n",
      "    accuracy                           0.41      5148\n",
      "   macro avg       0.39      0.46      0.41      5148\n",
      "weighted avg       0.39      0.41      0.39      5148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bi = [(x in ['far-left', 'far-right']) for x in y_train]\n",
    "y_test_bi = [(x in ['far-left', 'far-right']) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=30, min_impurity_decrease = 0.001, \n",
    "                               criterion = \"entropy\", n_estimators=1000, \n",
    "                               class_weight='balanced_subsample', random_state=0)\n",
    "model = model.fit(X_train,y_train_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8578088578088578\n",
      "F1-micro: 0.857808857808858\n",
      "F1-macro: 0.7867275789626196\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.87      0.91      4262\n",
      "        True       0.56      0.81      0.66       886\n",
      "\n",
      "    accuracy                           0.86      5148\n",
      "   macro avg       0.76      0.84      0.79      5148\n",
      "weighted avg       0.89      0.86      0.87      5148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model.predict(X_test), y_test_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EasyEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "model = EasyEnsembleClassifier(n_estimators=10,  random_state=0)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.37975912975912973\n",
      "F1-micro: 0.37975912975912973\n",
      "F1-macro: 0.4069889587469615\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "      center       0.38      0.75      0.50      1349\n",
      " center-left       0.24      0.14      0.17      1159\n",
      "center-right       0.27      0.12      0.16      1754\n",
      "    far-left       0.87      0.45      0.59       215\n",
      "   far-right       0.52      0.73      0.61       671\n",
      "\n",
      "    accuracy                           0.38      5148\n",
      "   macro avg       0.46      0.43      0.41      5148\n",
      "weighted avg       0.35      0.38      0.33      5148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty = 'l2', C = 0.01, solver = 'saga', random_state=0)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.44716394716394714\n",
      "F1-micro: 0.44716394716394714\n",
      "F1-macro: 0.26213295697953276\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "      center       0.41      0.20      0.27      1349\n",
      " center-left       0.37      0.49      0.42      1159\n",
      "center-right       0.50      0.84      0.62      1754\n",
      "    far-left       0.00      0.00      0.00       215\n",
      "   far-right       0.00      0.00      0.00       671\n",
      "\n",
      "    accuracy                           0.45      5148\n",
      "   macro avg       0.25      0.30      0.26      5148\n",
      "weighted avg       0.36      0.45      0.38      5148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha = 0.4)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4020979020979021\n",
      "F1-micro: 0.4020979020979021\n",
      "F1-macro: 0.22343815569135422\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "      center       0.42      0.12      0.19      1349\n",
      " center-left       0.31      0.42      0.35      1159\n",
      "center-right       0.45      0.81      0.58      1754\n",
      "    far-left       0.00      0.00      0.00       215\n",
      "   far-right       0.00      0.00      0.00       671\n",
      "\n",
      "    accuracy                           0.40      5148\n",
      "   macro avg       0.24      0.27      0.22      5148\n",
      "weighted avg       0.33      0.40      0.33      5148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daksenov/miniconda2/envs/summ/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(model.predict(X_test), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
